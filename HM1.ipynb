{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Stimulate Rankings of Relevance for E and P (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243\n",
      "(('HR', 'N', 'N', 'HR', 'R'), ('N', 'R', 'R', 'N', 'N'))\n",
      "(('N', 'R', 'N', 'HR', 'R'), ('HR', 'R', 'N', 'R', 'R'))\n",
      "(('R', 'R', 'HR', 'HR', 'HR'), ('N', 'R', 'HR', 'HR', 'R'))\n",
      "(('N', 'R', 'R', 'N', 'N'), ('R', 'N', 'N', 'HR', 'R'))\n",
      "(('R', 'N', 'HR', 'N', 'HR'), ('HR', 'HR', 'R', 'HR', 'N'))\n"
     ]
    }
   ],
   "source": [
    "import itertools as iter\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "#The gradings\n",
    "relevance_gradings = ['N', 'R', 'HR']\n",
    "\n",
    "# Create all possible versions of length five rankings\n",
    "list_p = [p for p in iter.product(relevance_gradings, repeat=5)]\n",
    "list_e = [p for p in iter.product(relevance_gradings, repeat=5)]\n",
    "\n",
    "print(len(list_e))\n",
    "\n",
    "# Create all possible pairs of the two lists - if computing power permits\n",
    "#list_pairs = [list(zip(list_p, p)) for p in itertools.permutations(list_e)]\n",
    "\n",
    "# Sampling - take one at random from list 1 and one 1 from list 2 until 1000.\n",
    "sample_size = 1000\n",
    "sample_list_p = [random.choice(list_p) for _ in range(sample_size)]\n",
    "sample_list_e = [random.choice(list_e) for _ in range(sample_size)]\n",
    "\n",
    "sample_pairs = list(zip(sample_list_p, sample_list_e))\n",
    "\n",
    "for i in range(0,5):\n",
    "    print(sample_pairs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Implement Evaluation Measures (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Measure - Average Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average precision across the queries for the production algorithm is: 0.5387533333333335\n",
      "The average precision across the queries for the experimental algorithm is: 0.5344400000000001\n"
     ]
    }
   ],
   "source": [
    "def calc_ave_precision(list_of_relevance):\n",
    "    rank_length = len(list_of_relevance)\n",
    "    running_score = 0\n",
    "    relevant_count = 0\n",
    "    for i in range(0, rank_length):\n",
    "        if(list_of_relevance[i] == \"R\" or list_of_relevance[i] == \"HR\"):\n",
    "            relevant_count += 1\n",
    "            running_score += relevant_count / (i+1)\n",
    "    ave_precision = running_score / rank_length\n",
    "    return ave_precision\n",
    "\n",
    "p_AP_scores = []\n",
    "e_AP_scores = []\n",
    "\n",
    "# Calculate average precision for all queries\n",
    "for pairs in sample_pairs:\n",
    "    p_AP_scores.append(calc_ave_precision(pairs[0]))\n",
    "    e_AP_scores.append(calc_ave_precision(pairs[1]))\n",
    "\n",
    "# Calculate the average of the average precisions across queries\n",
    "p_ave_precision_over_queries = sum(p_AP_scores) / len(p_AP_scores)\n",
    "e_ave_precision_over_queries = sum(e_AP_scores) / len(e_AP_scores)\n",
    "\n",
    "print(\"The average precision across the queries for the production algorithm is: %s\" % p_ave_precision_over_queries)\n",
    "print(\"The average precision across the queries for the experimental algorithm is: %s\" % e_ave_precision_over_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Multi-graded Evaluation Measure 1 - Discounted Cummulative Gain at rank k. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average DCG@5 across the queries for the production algorithm is: 30.874015525312785\n",
      "The average DCG@5 across the queries for the experimental algorithm is: 30.091236299530426\n"
     ]
    }
   ],
   "source": [
    "# Gain for each relevance label\n",
    "relevance_gain_dict = {'HR': 5, 'R': 1, 'N': 0}\n",
    "\n",
    "# Function that calculates DCG@5\n",
    "def DCG_Rank_K(ranked_list):\n",
    "    gain = 0\n",
    "    discounted_gain = 0\n",
    "    for i, item in enumerate(ranked_list):\n",
    "        rel = relevance_gain_dict[item]\n",
    "        discounted_gain += (2**rel - 1) /  (math.log2(i + 1 + 1))\n",
    "    return discounted_gain\n",
    "\n",
    "\n",
    "p_DCG_scores = []\n",
    "e_DCG_scores = []\n",
    "\n",
    "# Calculate DCG@5 for each algorithm on all queries\n",
    "for pairs in sample_pairs:\n",
    "    p_DCG_scores.append(DCG_Rank_K(pairs[0]))\n",
    "    e_DCG_scores.append(DCG_Rank_K(pairs[1]))\n",
    "\n",
    "# Calculate the average of the average precisions across queries\n",
    "p_ave_DCG_over_queries = sum(p_DCG_scores) / len(p_DCG_scores)\n",
    "e_ave_DCG_over_queries = sum(e_DCG_scores) / len(e_DCG_scores)\n",
    "\n",
    "print(\"The average DCG@5 across the queries for the production algorithm is: %s\" % p_ave_DCG_over_queries)\n",
    "print(\"The average DCG@5 across the queries for the experimental algorithm is: %s\" % e_ave_DCG_over_queries)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-graded Evaluation Measure 2 - Rank Biased Precision with persistence parameter $\\theta = 0.8$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average RBP across the queries for the production algorithm is: 8.682415632965938\n",
      "The average RBP across the queries for the experimental algorithm is: 8.437092224127818\n"
     ]
    }
   ],
   "source": [
    "# Gain for each relevance label\n",
    "relevance_gain_dict = {'HR': 5, 'R': 1, 'N': 0}\n",
    "\n",
    "# Function that calculates RBP\n",
    "def calc_RBP(ranked_list):\n",
    "    theta = 0.8\n",
    "    expected_utility = 0\n",
    "    for i, item in enumerate(ranked_list):\n",
    "        k_rel = relevance_gain_dict[item]\n",
    "        expected_utility += k_rel * pow(theta, (i+1)*(1-theta))\n",
    "    return expected_utility\n",
    "\n",
    "p_RBP_scores = []\n",
    "e_RBP_scores = []\n",
    "\n",
    "# Calculate RBP for each algorithm on all queries\n",
    "for pairs in sample_pairs:\n",
    "    p_RBP_scores.append(calc_RBP(pairs[0]))\n",
    "    e_RBP_scores.append(calc_RBP(pairs[1]))\n",
    "\n",
    "# Calculate the average of the average precisions across queries\n",
    "p_ave_RBP_over_queries = sum(p_RBP_scores) / len(p_RBP_scores)\n",
    "e_ave_RBP_over_queries = sum(e_RBP_scores) / len(e_RBP_scores)\n",
    "\n",
    "print(\"The average RBP across the queries for the production algorithm is: %s\" %p_ave_RBP_over_queries)\n",
    "print(\"The average RBP across the queries for the experimental algorithm is: %s\" %e_ave_RBP_over_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Calculate the $\\Delta measure$ (5 poins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to compare two lists of scores\n",
    "def compare_lists(list_a, list_b):\n",
    "    b_better_pairs = []\n",
    "    for i, item in enumerate(list_a):\n",
    "        if item < list_b[i]:\n",
    "            #b_better_pairs.append((list_b[i], item))\n",
    "            b_better_pairs.append(i)\n",
    "    return b_better_pairs\n",
    "\n",
    "\n",
    "# Average Precision Difference Measure\n",
    "E_better_AP = compare_lists(p_AP_scores, e_AP_scores)\n",
    "E_better_AP_Pairs = list(map((lambda x: sample_pairs[x]), E_better_AP))\n",
    "#print(E_better_AP_Pairs[0:5])\n",
    "\n",
    "# DCG@5 Difference Measure\n",
    "E_better_DCG = compare_lists(p_DCG_scores, e_DCG_scores)\n",
    "E_better_DCG_Pairs = list(map((lambda x: sample_pairs[x]), E_better_DCG))\n",
    "#print(E_better_DCG_Pairs[0:5])\n",
    "\n",
    "# RBP Difference Measure\n",
    "E_better_RBP = compare_lists(p_RBP_scores, e_RBP_scores)\n",
    "E_better_RBP_Pairs = list(map((lambda x: sample_pairs[x]), E_better_RBP))\n",
    "#print(E_better_RBP_Pairs[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Implement Interleaving (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced Interleaving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rankings from P: ('HR', 'N', 'N', 'HR', 'R') and Rankings from E: ('N', 'R', 'R', 'N', 'N')\n",
      "Interleaved with Balanced interleaving gives: ['N', 'HR', 'R', 'N', 'R', 'N', 'N', 'HR', 'N', 'R'] \n"
     ]
    }
   ],
   "source": [
    "# Function to interleave two ranked lists\n",
    "def interleave_Balanced(list_a, list_b, interleave_length=10):\n",
    "    # print(\"List A:\",list_a)\n",
    "    # print(\"List B:\",list_b)\n",
    "    interleaved = []\n",
    "    k_a = 1\n",
    "    k_b = 1\n",
    "\n",
    "    a_first = random.randint(0, 100) % 2\n",
    "    # print(\"B first\" if a_first == 0 else \"A first\")\n",
    "\n",
    "    # while (k_a <= (len(list_a)) and k_b <= len(list_b)):\n",
    "    while (len(interleaved) < (len(list_a) + len(list_b))):\n",
    "        if (k_a < k_b) or ((k_a == k_b) and a_first == 1):\n",
    "            # if (list_a[k_a-1] not in interleaved):\n",
    "            interleaved.append(list_a[k_a - 1])\n",
    "            k_a += 1\n",
    "        else:\n",
    "            # if (list_b[k_b-1] not in interleaved):\n",
    "            interleaved.append(list_b[k_b - 1])\n",
    "            k_b += 1\n",
    "\n",
    "            # print(interleaved, \"\\n\")\n",
    "    return interleaved\n",
    "\n",
    "\n",
    "# Interleave all sample pairs\n",
    "balanced_interleaved = []\n",
    "for pairs in sample_pairs:\n",
    "    balanced_interleaved.append(interleave_Balanced(pairs[0], pairs[1]))\n",
    "\n",
    "print(\"Rankings from P: %s and Rankings from E: %s\" % (sample_pairs[0][0], sample_pairs[0][1]))\n",
    "print(\"Interleaved with Balanced interleaving gives: %s \" % (balanced_interleaved[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic Interleaving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rankings from P: ('HR', 'N', 'N', 'HR', 'R') and Rankings from E: ('N', 'R', 'R', 'N', 'N')\n",
      "Interleaved with Probabilistic interleaving gives: ['N', 'HR', 'R', 'N', 'HR', 'N', 'R'] \n"
     ]
    }
   ],
   "source": [
    "class Interleave:\n",
    "\n",
    "    def soft_max(self, ranked_list, gamma):\n",
    "        selection_probabilities = []\n",
    "        normalizer = 0\n",
    "        for i in range(len(ranked_list)):\n",
    "            non_normalized_prob = 1 / (i + 1)**gamma\n",
    "            normalizer += non_normalized_prob\n",
    "            selection_probabilities.append(non_normalized_prob)\n",
    "        selection_probabilities = map(lambda x: x / normalizer, selection_probabilities)\n",
    "        return list(selection_probabilities)\n",
    "\n",
    "    def interleave_probabilistic(self, list_a, list_b, rank_length=10, gamma=3, clicks=False, click_prob=0):\n",
    "        # Control final list length, if total results are shorter than rank_length\n",
    "        if((len(list_a) + len(list_b)) < rank_length):\n",
    "            rank_length = len(list_a) if len(list_a) <= len(list_b) else len(list_b)        \n",
    "        \n",
    "        # The interleaved list to be presented to the user\n",
    "        interleaved = []\n",
    "        \n",
    "        # Lists of indices of each algorithms document in the interleaved list \n",
    "        a_indices = []\n",
    "        b_indices = []\n",
    "        \n",
    "        # Click counts for each algorithm\n",
    "        a_clicks = 0\n",
    "        b_clicks = 0\n",
    "\n",
    "        # Calculate probabilities for each document in each list\n",
    "        prob_a = self.soft_max(list_a, gamma)\n",
    "        prob_b = self.soft_max(list_b, gamma)\n",
    "\n",
    "        soft_max_selections_a = list(np.random.choice(list_a, len(list_a), p=prob_a, replace=False))\n",
    "        soft_max_selections_b = list(np.random.choice(list_b, len(list_b), p=prob_b, replace=False))\n",
    "\n",
    "        for i in range(rank_length):\n",
    "            if(len(soft_max_selections_a) > 0 and len(soft_max_selections_b) > 0):\n",
    "                choice = np.random.ranf()\n",
    "                if(choice <= 0.5):\n",
    "                    interleaved.append(soft_max_selections_a.pop(0))\n",
    "                    a_indices.append(i)\n",
    "                else:\n",
    "                    interleaved.append(soft_max_selections_b.pop(0))\n",
    "                    b_indices.append(i)\n",
    "\n",
    "            else:\n",
    "                # Now it stops if when of the lists run out. (Question)\n",
    "                break\n",
    "\n",
    "\n",
    "        if(clicks):\n",
    "            # Winner = 1 if B wins, else 0\n",
    "            winner = 0\n",
    "            for i, doc in enumerate(interleaved):\n",
    "                click = random.uniform(0, 1)\n",
    "                if(click <= click_prob):\n",
    "                    if(i in a_indices):\n",
    "                        a_clicks += 1\n",
    "                    else:\n",
    "                        b_clicks += 1\n",
    "            # Only if b strictly beats a (Question)\n",
    "            if(b_clicks > a_clicks): \n",
    "                winner = 1\n",
    "\n",
    "        if(clicks):\n",
    "            # If there is a click model, only output the winner (Question)\n",
    "            return winner\n",
    "        else:\n",
    "            return interleaved\n",
    "\n",
    "probabilistic_interleaver = Interleave()\n",
    "\n",
    "# Interleave all sample pairs\n",
    "probabilistic_interleaved = []\n",
    "for pairs in sample_pairs:\n",
    "    probabilistic_interleaved.append(probabilistic_interleaver.interleave_probabilistic(pairs[0], pairs[1]))\n",
    "\n",
    "print(\"Rankings from P: %s and Rankings from E: %s\" % (sample_pairs[0][0], sample_pairs[0][1])) \n",
    "print(\"Interleaved with Probabilistic interleaving gives: %s \" % (probabilistic_interleaved[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Implement User Clicks Simulation (25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Random Click Model (RCM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RCM trained on the Yandex Log file, gives a click probability p of 0.13445559411.\n"
     ]
    }
   ],
   "source": [
    "# All documents can be clicked with the same probability\n",
    "\n",
    "# The probability of being clicked is (number of clicks)/(number of documents shown)\n",
    "\n",
    "# This is what the model has to learn. \n",
    "headers = ['SessionID', 'TimePassed', 'TypeOfAction', 'QueryID', 'RegionID', \n",
    "'Rank1_URLID', \n",
    "'Rank2_URLID', \n",
    "'Rank3_URLID', \n",
    "'Rank4_URLID', \n",
    "'Rank5_URLID', \n",
    "'Rank6_URLID', \n",
    "'Rank7_URLID', \n",
    "'Rank8_URLID', \n",
    "'Rank9_URLID', \n",
    "'Rank10_URLID']\n",
    "\n",
    "data = pd.read_csv('YandexRelPredChallenge.txt', sep='\\t', names = headers)\n",
    "\n",
    "class Random_Click_Model:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.click_prob = 0\n",
    "\n",
    "    def random_click_model_learn(self, query_dataframe):\n",
    "        query_count = 0\n",
    "        click_prob_per_query = []\n",
    "\n",
    "        for index, row in query_dataframe.iterrows():\n",
    "            # Include session check\n",
    "            if(row['TypeOfAction'] == 'Q'):\n",
    "                query_count += 1\n",
    "                documents_shown = row[5:15].count()\n",
    "                click_count = 0\n",
    "                rows_below = 1\n",
    "                if((index + rows_below) >= len(query_dataframe.index)):\n",
    "                    click_prob_per_query.append(click_count / documents_shown)\n",
    "                    break\n",
    "                next_row_type = query_dataframe.iloc[index + rows_below]['TypeOfAction']\n",
    "                if(next_row_type == 'C'):\n",
    "                    while(next_row_type == 'C'):\n",
    "                        click_count += 1\n",
    "                        rows_below += 1\n",
    "                        if((index + rows_below) >= len(query_dataframe.index)):\n",
    "                            break\n",
    "                        else:\n",
    "                            next_row_type = query_dataframe.iloc[index + rows_below]['TypeOfAction']\n",
    "                    click_prob_per_query.append(click_count / documents_shown)\n",
    "                else:\n",
    "                    click_prob_per_query.append(click_count / documents_shown)\n",
    "        average_click_prob = sum(click_prob_per_query) / len(click_prob_per_query)\n",
    "        self.click_prob = average_click_prob\n",
    "\n",
    "random_click_model = Random_Click_Model()\n",
    "random_click_model.random_click_model_learn(data)\n",
    "\n",
    "print(\"The RCM trained on the Yandex Log file, gives a click probability p of %s.\" % random_click_model.click_prob)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified Dynamic Bayesian Network model (SDBN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New query, session ID:  0\n",
      "Retrived url ID_ 1 :  17562.0\n",
      "Retrived url ID_ 2 :  1627.0\n",
      "Retrived url ID_ 3 :  1626.0\n",
      "Retrived url ID_ 4 :  1623.0\n",
      "Retrived url ID_ 5 :  2091.0\n",
      "Retrived url ID_ 6 :  17559.0\n",
      "Retrived url ID_ 7 :  17563.0\n",
      "Retrived url ID_ 8 :  17558.0\n",
      "Retrived url ID_ 9 :  17561.0\n",
      "Retrived url ID_ 10 :  17560.0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SDBM' object has no attribute 'click_prob_per_query'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2b994c442f3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0msdbm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSDBM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'YandexRelPredChallenge.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msdbm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-2b994c442f3a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSDBN_model_learn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclick_prob_per_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-2b994c442f3a>\u001b[0m in \u001b[0;36mSDBN_model_learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclick_prob_per_query\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSDBM_model_click_propability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattractiveness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SDBM' object has no attribute 'click_prob_per_query'"
     ]
    }
   ],
   "source": [
    "# Simplified Dynamic Bayesian Network\n",
    "# Things I know for SDBN:\n",
    "# If E_r = 1 AND A_ur = 1 <=> C_r = 1\n",
    "# P(E_1 = 1) = 1 = att_1\n",
    "# P(E_r = 1 | E_r-1 = 0) = 0\n",
    "# P(E_r = 1 | S_r-1 = 1) = 0\n",
    "# P(E_r = 1 | E_r-1 = 1, S_r-1 = 0) = gamma = 1\n",
    "\n",
    "# Things I need to know for SDBN:\n",
    "# P(A_ur = 1) = alpha_urq\n",
    "# P(S_r = 1 | C_r = 1) = theta_urq\n",
    "# Examination at (rank - 1)\n",
    "# Attractiveness at rank\n",
    "# Satisfaction at rank\n",
    "# Click at rank\n",
    "class SDBM:\n",
    "    def __init__(self, file):\n",
    "        # This is what the model has to learn.\n",
    "        headers = ['SessionID', 'TimePassed', 'TypeOfAction', 'QueryID', 'RegionID',\n",
    "        'Rank1_URLID',\n",
    "        'Rank2_URLID',\n",
    "        'Rank3_URLID',\n",
    "        'Rank4_URLID',\n",
    "        'Rank5_URLID',\n",
    "        'Rank6_URLID',\n",
    "        'Rank7_URLID',\n",
    "        'Rank8_URLID',\n",
    "        'Rank9_URLID',\n",
    "        'Rank10_URLID']\n",
    "\n",
    "        self.data = pd.read_csv(file, sep='\\t', names = headers)\n",
    "        self.SDBN_model_learn()\n",
    "        self.click_prob_per_query = 0\n",
    "        \n",
    "    def SDBN_model_learn(self):\n",
    "        click_counter = 0\n",
    "\n",
    "        for index, row in self.data.iterrows():\n",
    "            next_row_type = self.data.iloc[index + 1]['TypeOfAction']\n",
    "            \n",
    "            # If it is a query followed by click: get list of retrived document id's, start counting clicks\n",
    "            if (row['TypeOfAction'] == 'Q'):\n",
    "                click_counter = 0\n",
    "                retrived_url_IDs = 0\n",
    "                if (next_row_type == 'C'):                       \n",
    "                    retrived_url_IDs = row.values[5:15]\n",
    "                    print(\"New query, session ID: \", self.data.iloc[index]['SessionID'])\n",
    "                    for i in range(0,10):\n",
    "                        print(\"Retrived url ID_\", (i+1), \": \", retrived_url_IDs[i])\n",
    "            else:\n",
    "                if (next_row_type == 'Q'):\n",
    "                    satisfaction = 1\n",
    "                    print(\"Satisfied after \", click_counter, \"clicks. Sessie ID: \",\n",
    "                          self.data.iloc[index]['SessionID'], \"\\n\")\n",
    "                else:\n",
    "                    satisfaction = 0\n",
    "                \n",
    "                attractiveness = 1\n",
    "                document_url_id = row.values[3]\n",
    "                rank = np.where(retrived_url_IDs == document_url_id)[0]+1\n",
    "\n",
    "                \n",
    "                self.click_prob_per_query.append(self.SDBM_model_click_propability(self, attractiveness, rank))\n",
    "                \n",
    "                \n",
    "                click_counter += 1\n",
    "                print(\"Click nr.: \", click_counter, \"urlID: \", row.values[3], \" rank in query: \", rank)\n",
    "    \n",
    "        average_click_prob = sum(self.click_prob_per_query) / len(self.click_prob_per_query)\n",
    "        return average_click_prob \n",
    "\n",
    "\n",
    "    def SDBM_model_click_propability(self, attractiveness, rank):\n",
    "        return (attractiveness * SDBN_model_examination_propability(rank, satisfaction, attractiveness))\n",
    "\n",
    "    def SDBN_model_examination_propability(self, rank, satisfaction, attractiveness, gamma = 1):\n",
    "        if(rank == 1):\n",
    "            return 1\n",
    "        else:\n",
    "            return (SDBN_model_examination_propabability((r-1), satisfaction, attractiveness) * gamma \n",
    "                    * (attractiveness * (1 - satisfaction) + (1 - attractiveness)))\n",
    "\n",
    "\n",
    "sdbm = SDBM('YandexRelPredChallenge.txt')\n",
    "\n",
    "print (sdbm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Simulate Interleaving Experiment (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "winners = []\n",
    "for pairs in sample_pairs:\n",
    "    winners.append(probabilistic_interleaver.interleave_probabilistic(pairs[0], \n",
    "                                                                      pairs[1], \n",
    "                                                                      clicks=True, \n",
    "                                                                      click_prob=random_click_model.click_prob))\n",
    "    \n",
    "print(\"The proportion of E wins over P across the 1000 samples is: %s\" % (sum(winners)/len(winners)))\n",
    "\n",
    "winners_AP_compare = []\n",
    "for pairs in E_better_AP_Pairs:\n",
    "    winners.append(probabilistic_interleaver.interleave_probabilistic(pairs[0], \n",
    "                                                                      pairs[1], \n",
    "                                                                      clicks=True, \n",
    "                                                                      click_prob=random_click_model.click_prob))\n",
    "    \n",
    "print(\"The proportion of E wins over P across the samples where E has a better AP is: %s\" % (sum(winners)/len(winners)))\n",
    "\n",
    "# Implement further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Results and Analyis (25 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
