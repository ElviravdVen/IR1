{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Stimulate Rankings of Relevance for E and P (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243\n",
      "(('HR', 'R', 'N', 'R', 'R'), ('HR', 'N', 'HR', 'HR', 'N'))\n",
      "(('HR', 'HR', 'HR', 'N', 'HR'), ('HR', 'HR', 'HR', 'N', 'R'))\n",
      "(('HR', 'R', 'N', 'N', 'HR'), ('N', 'N', 'N', 'N', 'R'))\n",
      "(('HR', 'R', 'N', 'HR', 'R'), ('N', 'N', 'R', 'HR', 'HR'))\n",
      "(('R', 'R', 'HR', 'R', 'N'), ('HR', 'HR', 'R', 'HR', 'N'))\n"
     ]
    }
   ],
   "source": [
    "import itertools as iter\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "#The gradings\n",
    "relevance_gradings = ['N', 'R', 'HR']\n",
    "\n",
    "# Create all possible versions of length five rankings\n",
    "list_p = [p for p in iter.product(relevance_gradings, repeat=5)]\n",
    "list_e = [p for p in iter.product(relevance_gradings, repeat=5)]\n",
    "\n",
    "print(len(list_e))\n",
    "\n",
    "# Create all possible pairs of the two lists - if computing power permits\n",
    "#list_pairs = [list(zip(list_p, p)) for p in itertools.permutations(list_e)]\n",
    "\n",
    "# Sampling - take one at random from list 1 and one 1 from list 2 until 1000.\n",
    "sample_size = 1000\n",
    "sample_list_p = [random.choice(list_p) for _ in range(sample_size)]\n",
    "sample_list_e = [random.choice(list_e) for _ in range(sample_size)]\n",
    "\n",
    "sample_pairs = list(zip(sample_list_p, sample_list_e))\n",
    "\n",
    "for i in range(0,5):\n",
    "    print(sample_pairs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Implement Evaluation Measures (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Measure - Average Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average precision across the queries for the production algorithm is: 0.5387166666666665\n",
      "The average precision across the queries for the experimental algorithm is: 0.5604966666666659\n"
     ]
    }
   ],
   "source": [
    "def calc_ave_precision(list_of_relevance):\n",
    "    rank_length = len(list_of_relevance)\n",
    "    running_score = 0\n",
    "    relevant_count = 0\n",
    "    for i in range(0, rank_length):\n",
    "        if(list_of_relevance[i] == \"R\" or list_of_relevance[i] == \"HR\"):\n",
    "            relevant_count += 1\n",
    "            running_score += relevant_count / (i+1)\n",
    "    ave_precision = running_score / rank_length\n",
    "    return ave_precision\n",
    "\n",
    "p_AP_scores = []\n",
    "e_AP_scores = []\n",
    "\n",
    "# Calculate average precision for all queries\n",
    "for pairs in sample_pairs:\n",
    "    p_AP_scores.append(calc_ave_precision(pairs[0]))\n",
    "    e_AP_scores.append(calc_ave_precision(pairs[1]))\n",
    "\n",
    "# Calculate the average of the average precisions across queries\n",
    "p_ave_precision_over_queries = sum(p_AP_scores) / len(p_AP_scores)\n",
    "e_ave_precision_over_queries = sum(e_AP_scores) / len(e_AP_scores)\n",
    "\n",
    "print(\"The average precision across the queries for the production algorithm is: %s\" % p_ave_precision_over_queries)\n",
    "print(\"The average precision across the queries for the experimental algorithm is: %s\" % e_ave_precision_over_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Multi-graded Evaluation Measure 1 - Discounted Cummulative Gain at rank k. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average DCG@5 across the queries for the production algorithm is: 32.31213721006813\n",
      "The average DCG@5 across the queries for the experimental algorithm is: 32.41057379341242\n"
     ]
    }
   ],
   "source": [
    "# Gain for each relevance label\n",
    "relevance_gain_dict = {'HR': 5, 'R': 1, 'N': 0}\n",
    "\n",
    "# Function that calculates DCG@5\n",
    "def DCG_Rank_K(ranked_list):\n",
    "    gain = 0\n",
    "    discounted_gain = 0\n",
    "    for i, item in enumerate(ranked_list):\n",
    "        rel = relevance_gain_dict[item]\n",
    "        discounted_gain += (2**rel - 1) /  (math.log2(i + 1 + 1))\n",
    "    return discounted_gain\n",
    "\n",
    "\n",
    "p_DCG_scores = []\n",
    "e_DCG_scores = []\n",
    "\n",
    "# Calculate DCG@5 for each algorithm on all queries\n",
    "for pairs in sample_pairs:\n",
    "    p_DCG_scores.append(DCG_Rank_K(pairs[0]))\n",
    "    e_DCG_scores.append(DCG_Rank_K(pairs[1]))\n",
    "\n",
    "# Calculate the average of the average precisions across queries\n",
    "p_ave_DCG_over_queries = sum(p_DCG_scores) / len(p_DCG_scores)\n",
    "e_ave_DCG_over_queries = sum(e_DCG_scores) / len(e_DCG_scores)\n",
    "\n",
    "print(\"The average DCG@5 across the queries for the production algorithm is: %s\" % p_ave_DCG_over_queries)\n",
    "print(\"The average DCG@5 across the queries for the experimental algorithm is: %s\" % e_ave_DCG_over_queries)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-graded Evaluation Measure 2 - Rank Biased Precision with persistence parameter $\\theta = 0.8$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average RBP across the queries for the production algorithm is: 8.933936315806799\n",
      "The average RBP across the queries for the experimental algorithm is: 8.991956475111728\n"
     ]
    }
   ],
   "source": [
    "# Gain for each relevance label\n",
    "relevance_gain_dict = {'HR': 5, 'R': 1, 'N': 0}\n",
    "\n",
    "# Function that calculates RBP\n",
    "def calc_RBP(ranked_list):\n",
    "    theta = 0.8\n",
    "    expected_utility = 0\n",
    "    for i, item in enumerate(ranked_list):\n",
    "        k_rel = relevance_gain_dict[item]\n",
    "        expected_utility += k_rel * pow(theta, (i+1)*(1-theta))\n",
    "    return expected_utility\n",
    "\n",
    "p_RBP_scores = []\n",
    "e_RBP_scores = []\n",
    "\n",
    "# Calculate RBP for each algorithm on all queries\n",
    "for pairs in sample_pairs:\n",
    "    p_RBP_scores.append(calc_RBP(pairs[0]))\n",
    "    e_RBP_scores.append(calc_RBP(pairs[1]))\n",
    "\n",
    "# Calculate the average of the average precisions across queries\n",
    "p_ave_RBP_over_queries = sum(p_RBP_scores) / len(p_RBP_scores)\n",
    "e_ave_RBP_over_queries = sum(e_RBP_scores) / len(e_RBP_scores)\n",
    "\n",
    "print(\"The average RBP across the queries for the production algorithm is: %s\" %p_ave_RBP_over_queries)\n",
    "print(\"The average RBP across the queries for the experimental algorithm is: %s\" %e_ave_RBP_over_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Calculate the $\\Delta measure$ (5 poins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to compare two lists of scores\n",
    "def compare_lists(list_a, list_b):\n",
    "    b_better_pairs = []\n",
    "    for i, item in enumerate(list_a):\n",
    "        if item < list_b[i]:\n",
    "            #b_better_pairs.append((list_b[i], item))\n",
    "            b_better_pairs.append(i)\n",
    "    return b_better_pairs\n",
    "\n",
    "\n",
    "# Average Precision Difference Measure\n",
    "E_better_AP = compare_lists(p_AP_scores, e_AP_scores)\n",
    "E_better_AP_Pairs = list(map((lambda x: sample_pairs[x]), E_better_AP))\n",
    "#print(E_better_AP_Pairs[0:5])\n",
    "\n",
    "# DCG@5 Difference Measure\n",
    "E_better_DCG = compare_lists(p_DCG_scores, e_DCG_scores)\n",
    "E_better_DCG_Pairs = list(map((lambda x: sample_pairs[x]), E_better_DCG))\n",
    "#print(E_better_DCG_Pairs[0:5])\n",
    "\n",
    "# RBP Difference Measure\n",
    "E_better_RBP = compare_lists(p_RBP_scores, e_RBP_scores)\n",
    "E_better_RBP_Pairs = list(map((lambda x: sample_pairs[x]), E_better_RBP))\n",
    "#print(E_better_RBP_Pairs[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Implement Interleaving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Implement User Clicks Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Simulate Interleaving Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Results and Analyis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
