{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Stimulate Rankings of Relevance for E and P (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243\n",
      "(('N', 'R', 'N', 'HR', 'R'), ('N', 'HR', 'N', 'R', 'R'))\n",
      "(('HR', 'HR', 'R', 'N', 'N'), ('R', 'N', 'R', 'N', 'HR'))\n",
      "(('HR', 'R', 'R', 'R', 'R'), ('N', 'HR', 'N', 'N', 'HR'))\n",
      "(('R', 'R', 'N', 'N', 'HR'), ('R', 'R', 'R', 'HR', 'R'))\n",
      "(('R', 'N', 'HR', 'R', 'R'), ('N', 'R', 'R', 'N', 'R'))\n"
     ]
    }
   ],
   "source": [
    "import itertools as iter\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#The gradings\n",
    "relevance_gradings = ['N', 'R', 'HR']\n",
    "\n",
    "# Create all possible versions of length five rankings\n",
    "list_p = [p for p in iter.product(relevance_gradings, repeat=5)]\n",
    "list_e = [p for p in iter.product(relevance_gradings, repeat=5)]\n",
    "\n",
    "print(len(list_e))\n",
    "\n",
    "# Create all possible pairs of the two lists - if computing power permits\n",
    "#list_pairs = [list(zip(list_p, p)) for p in itertools.permutations(list_e)]\n",
    "\n",
    "# Sampling - take one at random from list 1 and one 1 from list 2 until 1000.\n",
    "sample_size = 1000\n",
    "sample_list_p = [random.choice(list_p) for _ in range(sample_size)]\n",
    "sample_list_e = [random.choice(list_e) for _ in range(sample_size)]\n",
    "\n",
    "sample_pairs = list(zip(sample_list_p, sample_list_e))\n",
    "\n",
    "for i in range(0,5):\n",
    "    print(sample_pairs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Implement Evaluation Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('HR', 'R', 'R', 'R', 'R')\n",
      "1.0\n",
      "('N', 'HR', 'N', 'N', 'HR')\n",
      "0.18\n"
     ]
    }
   ],
   "source": [
    "# Binary - Average Precision\n",
    "def calc_ave_precision(list_of_relevance):\n",
    "    rank_length = len(list_of_relevance)\n",
    "    running_score = 0\n",
    "    relevant_count = 0\n",
    "    for i in range(0, rank_length):\n",
    "        if(list_of_relevance[i] == \"R\" or list_of_relevance[i] == \"HR\"):\n",
    "            relevant_count += 1\n",
    "            running_score += relevant_count / (i + 1)  \n",
    "    ave_precision = running_score / rank_length\n",
    "    return ave_precision\n",
    "\n",
    "test_p = sample_pairs[2][0]\n",
    "test_e = sample_pairs[2][1]\n",
    "\n",
    "print(test_p)\n",
    "print(calc_ave_precision(test_p))\n",
    "print(test_e)\n",
    "print(calc_ave_precision(test_e))\n",
    "    \n",
    "# Multi-graded 1\n",
    "\n",
    "\n",
    "# Multi-graded 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
