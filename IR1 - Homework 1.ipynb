{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Stimulate Rankings of Relevance for E and P (5 points)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 23,
>>>>>>> 7bc3eb6c781141738e1aa8f074c60ef50dbf37ad
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243\n",
<<<<<<< HEAD
      "(('N', 'R', 'N', 'HR', 'R'), ('N', 'HR', 'N', 'R', 'R'))\n",
      "(('HR', 'HR', 'R', 'N', 'N'), ('R', 'N', 'R', 'N', 'HR'))\n",
      "(('HR', 'R', 'R', 'R', 'R'), ('N', 'HR', 'N', 'N', 'HR'))\n",
      "(('R', 'R', 'N', 'N', 'HR'), ('R', 'R', 'R', 'HR', 'R'))\n",
      "(('R', 'N', 'HR', 'R', 'R'), ('N', 'R', 'R', 'N', 'R'))\n"
=======
      "(('N', 'HR', 'HR', 'R', 'N'), ('HR', 'N', 'R', 'N', 'HR'))\n",
      "(('N', 'HR', 'HR', 'R', 'HR'), ('N', 'N', 'N', 'HR', 'R'))\n",
      "(('R', 'N', 'N', 'N', 'HR'), ('N', 'R', 'N', 'HR', 'R'))\n",
      "(('R', 'N', 'N', 'N', 'HR'), ('HR', 'N', 'N', 'HR', 'N'))\n",
      "(('HR', 'HR', 'HR', 'R', 'N'), ('R', 'HR', 'N', 'R', 'N'))\n",
      "0.160512805254\n"
>>>>>>> 7bc3eb6c781141738e1aa8f074c60ef50dbf37ad
     ]
    }
   ],
   "source": [
    "import itertools as iter\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#The gradings\n",
    "relevance_gradings = ['N', 'R', 'HR']\n",
    "\n",
    "# Create all possible versions of length five rankings\n",
    "list_p = [p for p in iter.product(relevance_gradings, repeat=5)]\n",
    "list_e = [p for p in iter.product(relevance_gradings, repeat=5)]\n",
    "\n",
    "print(len(list_e))\n",
    "\n",
    "# Create all possible pairs of the two lists - if computing power permits\n",
    "#list_pairs = [list(zip(list_p, p)) for p in itertools.permutations(list_e)]\n",
    "\n",
    "# Sampling - take one at random from list 1 and one 1 from list 2 until 1000.\n",
    "sample_size = 1000\n",
    "sample_list_p = [random.choice(list_p) for _ in range(sample_size)]\n",
    "sample_list_e = [random.choice(list_e) for _ in range(sample_size)]\n",
    "\n",
    "sample_pairs = list(zip(sample_list_p, sample_list_e))\n",
    "\n",
    "for i in range(0,5):\n",
    "    print(sample_pairs[i])\n",
    "\n",
    "#Implement the evaluation Measures\n",
    "#Implement 1 binary and 2 multi-graded evaluation meassures out of the 7 measures mentioned below:\n",
    "#Binary evaluation measures:\n",
    "    #Precision at rank k\n",
    "    #Recall at rank k\n",
    "    #Average Precision\n",
    "#Multi-graded evaluation measures:\n",
    "    #Discounted Cumultative Gain at rank k (DCG@k)\n",
    "    #Normalized Discounted Cumulative Gain at rank k (nDCG@k)\n",
    "    #Rank Biased Precision with persistence parameter &theta; = 0.8(RBP)\n",
    "    #Expected Reciprocal Rank (ERR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Implement Evaluation Measures"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
=======
   "execution_count": 32,
>>>>>>> 7bc3eb6c781141738e1aa8f074c60ef50dbf37ad
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "('HR', 'R', 'R', 'R', 'R')\n",
      "1.0\n",
      "('N', 'HR', 'N', 'N', 'HR')\n",
      "0.18\n"
=======
      "0.160512805254\n"
>>>>>>> 7bc3eb6c781141738e1aa8f074c60ef50dbf37ad
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "# Binary - Average Precision\n",
    "def calc_ave_precision(list_of_relevance):\n",
    "    rank_length = len(list_of_relevance)\n",
    "    running_score = 0\n",
    "    relevant_count = 0\n",
    "    for i in range(0, rank_length):\n",
    "        if(list_of_relevance[i] == \"R\" or list_of_relevance[i] == \"HR\"):\n",
    "            relevant_count += 1\n",
    "            running_score += relevant_count / (i + 1)  \n",
    "    ave_precision = running_score / rank_length\n",
    "    return ave_precision\n",
    "\n",
    "test_p = sample_pairs[2][0]\n",
    "test_e = sample_pairs[2][1]\n",
=======
    "# Binary average precision\n",
    "def apk(sample_pairs):\n",
    "    \"\"\"\n",
    "    Average prescision between two lists of\n",
    "    items.\n",
    "    \"\"\"\n",
    "    actual = zip(*sample_pairs)[0]\n",
    "    predicted = zip(*sample_pairs)[1]\n",
    "    \n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "    \n",
    "    return score / min(len(actual), len(sample_pairs))\n",
    "\n",
    "print(apk(sample_pairs))\n",
>>>>>>> 7bc3eb6c781141738e1aa8f074c60ef50dbf37ad
    "\n",
    "print(test_p)\n",
    "print(calc_ave_precision(test_p))\n",
    "print(test_e)\n",
    "print(calc_ave_precision(test_e))\n",
    "    \n",
    "# Multi-graded 1\n",
    "\n",
    "\n",
    "# Multi-graded 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
<<<<<<< HEAD
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
=======
>>>>>>> 7bc3eb6c781141738e1aa8f074c60ef50dbf37ad
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
<<<<<<< HEAD
   "name": "python3"
=======
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
>>>>>>> 7bc3eb6c781141738e1aa8f074c60ef50dbf37ad
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
